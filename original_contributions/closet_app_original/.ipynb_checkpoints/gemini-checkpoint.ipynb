{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path 1 - Gemini API\n",
    "This path will guide you on the set-up and usage of Google Gemini's API.\n",
    "\n",
    "### 0. Get and store the API key\n",
    "First of all, you need to login with your google account and get an API key [here](https://aistudio.google.com/app/apikey). It is **very important** that you do not share your API key with anyone and that you do not have it in your Repository.\n",
    "\n",
    "You can keep your API key in a secure local document and access it when needed. It is common to save the key as an environmental variable so that it can be accessed by your python script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this means your API key is in plain text in your script. To avoid this, if you're using VS Code, you can add your API key to a `.env` file in your workspace root with the following line:\n",
    "\n",
    "```sh\n",
    "GEMINI_API_KEY=\"PASTE YOUR KEY HERE\"\n",
    "```\n",
    "\n",
    "Alternatively, you can use the [dot-env library](https://github.com/theskumar/python-dotenv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check if the environment variable API_KEY has been set up properly by running this line\n",
    "!if [ -z $GEMINI_API_KEY ]; then echo \"\\$GEMINI_API_KEY not found\"; else echo \"\\$GEMINI_API_KEY found\"; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. First simple request\n",
    "Now, you can write a simple script to see if everything is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "# The client gets the API key from the environment variable `GEMINI_API_KEY`.\n",
    "client = genai.Client()  # here you can also pass the api_key directly using os.environ['GEMINI_API_KEY']\n",
    "\n",
    "default_model = \"gemini-2.5-flash\"\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "\n",
    "Ask the model to generate content about a random topic and print the response in text.\n",
    "\n",
    "Here is the [official documentation](https://ai.google.dev/gemini-api/docs/text-generation?lang=python#configure) to find the help you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generation parameters\n",
    "\n",
    "When asking the model to generate some text, there are different parameters that you can tune to improve on the final quality of the text. [Here](https://ai.google.dev/gemini-api/docs/models/generative-models#model-parameters) is an overview of the parameters that Gemini offers. Try some of them in different context and understand how they affect the final generated text.\n",
    "\n",
    "#### Exercise 2\n",
    "\n",
    "Play with the output temperature, which controls the randomness of the generated text `temperature=0` means deterministic output, while `temperature=1` means maximum randomness (try some intermediate value too). Consider keeping the `max_output_tokens` to 50 so that the output is not too long; if you do, you should also set a low `thinking_budget` to avoid an empty response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.genai import types\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "\n",
    "Try out different `top_k` values, which controls how many tokens the model considers for output `top_k=1` means the model considers only one token for output (the one with the highest probability) `top_k=50` means the model considers the top 50 tokens for output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "\n",
    "The same exercise as before but now with `top_p`, which controls how the model selects tokens for output `top_p=0.1` means the model selects tokens that make up 10% of the cumulative probability mass `top_p=0.9` means the model selects tokens that make up 90% of the cumulative probability mass `top_p` filters tokens *after* applying `top_k`.\n",
    "\n",
    "Can you determine a rule of thumb as to how `top_k` and `top_p` affect the output results? (If you can't try to push the values to extreme values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Add images to the prompt\n",
    "\n",
    "#### Exercise 5\n",
    "Gemini, beside text also accepts images (and videos). Try prompting it with one. Choose an interesting image and prompt the model with a query about it.\n",
    "\n",
    "You can use the [official documentation](https://ai.google.dev/gemini-api/docs/vision?lang=python#prompting-images).\n",
    "\n",
    "Use [PIL](https://pillow.readthedocs.io/en/stable/) to load an image. It should already be present in the Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "IMAGE_PATH = \"./data/engineer_fitting_prosthetic_arm.jpg\"\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Retrieval Augmented Generation (RAG)\n",
    "\n",
    "#### Exercise 6\n",
    "\n",
    "Depending on the application of the project, you might need to extract text from given documents and include it as additional context. This becomes especially relevant if you have many documents that cannot possibly fit into the model's context window. To more easily implement a RAG pipeline we recommend the use of one of these libraries: [LangChain](https://python.langchain.com/v0.2/docs/introduction/), [LlamaIndex](https://docs.llamaindex.ai/en/stable/examples/), [Haystack](https://docs.haystack.deepset.ai/docs/intro).\n",
    "\n",
    "For the solution of this lab we will use *LangChain*.\n",
    "\n",
    "It can be useful to split this exercise into these steps:\n",
    "1. Read one or more documents using pdfminer\n",
    "2. Split the documents into small chunks\n",
    "3. Get and store the embeddings for each chunks\n",
    "5. Given a query, retrieve the most relevant chunk(s) and appropriately prompt your LLM\n",
    "\n",
    "**NOTE:** if you try to embed too many documents at once or too large documents you may run into rate limits. Possible solutions: \n",
    "* Reduce the number of chunks and/or their size\n",
    "* Look at the HF version of this lab and use a local embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # langchain expects gemini's api key to be in the environment variable GOOGLE_API_KEY, use os to set it\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings  # get embeddings from Gemini\n",
    "from langchain_community.vectorstores import FAISS  # \"db\" to store and retrieve embeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # split long documents\n",
    "from pdfminer.high_level import extract_text  # extract text from pdfs\n",
    "\n",
    "DOC_PATH = \"./data/chain_of_thought_prompting.pdf\"\n",
    "\n",
    "# Suppose a user query\n",
    "USER_QUERY = \"What is CoT?\"\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Explore on your own\n",
    "Gemini offers a bigger range of capabilities than those provided here, begin able to automatically handle multi-turn chats is one of them. Explore them on your own!\n",
    "\n",
    "#### Exercise 7\n",
    "Explore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Create a user interface\n",
    "\n",
    "#### Exercise 8\n",
    "Since you are trying to build a complete application, you also need a nice user interface that interacts with the model. There are various libraries available for this purpose. Notably: [gradio](https://www.gradio.app/docs/gradio/interface) and [chat UI](https://huggingface.co/docs/chat-ui/index). For the solution of this lab, we will use gradio.\n",
    "\n",
    "Gradio has pre-defined input/output blocks that are automatically inserted in the interface. You only need to provide an appropriate function that takes all the inputs and returns the relevant output. See documentation [here](https://www.gradio.app/docs/gradio/interface).\n",
    "\n",
    "Use a ChatInterface to create a chatbot UI that let's you discuss with Gemini, then add multimodal capabilities for both Gradio and Gemini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# This part closes the demo server if it is already running (which\n",
    "# happens easily in notebooks) and prevents you from opening multiple\n",
    "# servers at the same time.\n",
    "if \"demo\" in locals() and demo.is_running:\n",
    "    demo.close()\n",
    "\n",
    "# Edit the parameters below\n",
    "chats = {}  # store the chat history for each user (suppose multiple users)\n",
    "\n",
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
